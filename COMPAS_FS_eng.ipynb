{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3316e72b-7fe1-4fde-a0b7-15bb8c332db8",
   "metadata": {},
   "source": [
    "# Yet Another ProPublica COMPAS notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d9b45-1a4e-4ce7-950c-64a9fe8f4271",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e5077-6e29-486f-846f-c2c4329b2b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "!pip install fairlearn \n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/javism/seminariofate2025/main/data/propublica_recidivism_train.csv')\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/javism/seminariofate2025/main/data/propublica_recidivism_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6454c76-2955-47e8-a6ee-c16d004338b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d85958-0a89-4ab2-ad61-a2aebb56bc00",
   "metadata": {},
   "source": [
    "We plot the histogram of the scores by \"race\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68529d77-8a7a-4018-b84e-9b733fe03e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['race_label'] = 'black'\n",
    "df.loc[df['race'] == 0, 'race_label'] = 'white'\n",
    "df_test['race_label'] = 'black'\n",
    "df_test.loc[df_test['race'] == 0, 'race_label'] = 'white'\n",
    "\n",
    "df['decile_score'].hist(by=df['race_label'], figsize = (8,3))\n",
    "df['y'].hist(by=df['race_label'], figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c34f0-b0cc-47dc-badf-0ddd2676a6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def z_score_standardization(series):\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "df_s = df.copy()\n",
    "for col in df_s.columns:\n",
    "    if col != 'race_label':\n",
    "        df_s[col] = z_score_standardization(df_s[col])\n",
    "    \n",
    "df_s.groupby('race_label').boxplot(rot=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df9eee-3c5c-47c6-9680-a44eb568a4b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008b50f-fcd5-47e2-a9c8-5c05b205247d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove some variables, standarize and binarize label\n",
    "race_label = df['race_label']\n",
    "race_label_t = df_test['race_label']\n",
    "df.drop(['c_charge_desc','decile_score','score_text', 'race_label'], axis=1, inplace=True)\n",
    "df_test.drop(['c_charge_desc','decile_score','score_text', 'race_label'], axis=1, inplace=True)\n",
    "\n",
    "X = df.drop(['y'], axis=1)\n",
    "feature_names = X.columns\n",
    "y = df['y']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = df_test.drop(['y'], axis=1)\n",
    "y_test = df_test['y']\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798f0fc-b126-4be9-b852-50b0c608e4f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Disparity metrics auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e86a6-44ec-45ab-b8ae-fcb560bb32d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from fairlearn.metrics import false_positive_rate\n",
    "from fairlearn.metrics import false_negative_rate\n",
    "from fairlearn.metrics import true_positive_rate\n",
    "from fairlearn.metrics import selection_rate\n",
    "from fairlearn.metrics import count\n",
    "\n",
    "def performance_metrics(y_test, predictions, race_label_t): \n",
    "    # Test for discrimination in predictions\n",
    "    from fairlearn.metrics import MetricFrame,false_negative_rate,false_positive_rate\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    grouped_metric = MetricFrame(metrics=accuracy_score,y_true=y_test,y_pred=predictions, sensitive_features=race_label_t)\n",
    "    print(f\"Overall Accuracy = \\t{grouped_metric.overall:.4f}\")\n",
    "    print(\"Accuracy by groups:\")\n",
    "    for key, value in grouped_metric.by_group.to_dict().items():\n",
    "        print(f\"Acc {key}: \\t\\t{value:.4f}\")\n",
    "\n",
    "    grouped_metric = MetricFrame(metrics=false_negative_rate,y_true=y_test,y_pred=predictions, sensitive_features=race_label_t)\n",
    "    print(f\"Overall FNR = \\t\\t{grouped_metric.overall:.4f}\")\n",
    "    print(\"FNR by groups: \")\n",
    "    for key, value in grouped_metric.by_group.to_dict().items():\n",
    "        print(f\"FNR {key}: \\t\\t{value:.4f}\")\n",
    "\n",
    "    grouped_metric = MetricFrame(metrics=false_positive_rate,y_true=y_test,y_pred=predictions, sensitive_features=race_label_t)\n",
    "    print(f\"Overall FPR = \\t\\t{grouped_metric.overall:.4f}\")\n",
    "    print(\"FPR by groups: \")\n",
    "    for key, value in grouped_metric.by_group.to_dict().items():\n",
    "        print(f\"\\x1B[1mFPR {key}: \\t\\t{value:.4f}\")\n",
    "        \n",
    "\n",
    "def performance_plots(y_test, predictions, race_label_t): \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score,\n",
    "        'precision': precision_score,\n",
    "        'recall': recall_score,\n",
    "        'false positive rate': false_positive_rate,\n",
    "        'false negative rate': false_negative_rate,\n",
    "        'count': count}\n",
    "    metric_frame = MetricFrame(metrics=metrics,\n",
    "                               y_true=y_test,\n",
    "                               y_pred=predictions,\n",
    "                               sensitive_features=race_label_t)\n",
    "    metric_frame.by_group.plot.bar(\n",
    "        subplots=True,\n",
    "        layout=[3, 3],\n",
    "        legend=False,\n",
    "        figsize=[12, 8],\n",
    "        title=\"LR\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac8f9c5-dd1c-40ee-9af3-25272e214e53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic model fitting and performance evaluation\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> What conclusions do you draw from the performance by groups?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3c7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a logistic regression predictor\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "lr.score(X_test, y_test)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "performance_metrics(y_test, predictions, race_label_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7539d-fa1a-4865-a988-df05284f0971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_plots(y_test, predictions, race_label_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc0605-f2b9-4e77-b83a-e6923f3e11aa",
   "metadata": {},
   "source": [
    "## Print the linear part of our mathematical model\n",
    "\n",
    "<img src=\"pics/compas_diagram_pre.png\" alt=\"COMPAS model as a graph\" class=\"bg-primary\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca677589-418a-417f-b199-c5a74157a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = lr.intercept_[0];\n",
    "print(f'{intercept:.3f} ', end=\"\")\n",
    "for c, f in zip(lr.coef_.ravel(), list(feature_names) ):\n",
    "    print(f'{c:.3f} * {f}', end=' + ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba19579",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac20ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "print(X.shape)\n",
    "\n",
    "lsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X = model.transform(X)\n",
    "X_test = model.transform(X_test)\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(f\"Features selected by SelectFromModel: {feature_names[model.get_support()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31b476-8e6d-485a-bee8-476f75a82b71",
   "metadata": {},
   "source": [
    "## Fitting the model with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd30763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a logistic regression predictor\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "lr.score(X_test, y_test)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "performance_metrics(y_test, predictions, race_label_t)\n",
    "performance_plots(y_test, predictions, race_label_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91e581-aa75-46a4-96ed-cab058d9bce2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Print the linear part of our mathematical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea357228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intercept = lr.intercept_[0];\n",
    "print(f'{intercept:.3f} ', end=\"\")\n",
    "for c, f in zip(lr.coef_.ravel(), list(feature_names[model.get_support()]) ):\n",
    "    print(f'{c:.3f} * {f}', end=' + ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36745b04-58dc-4be2-b7fd-27119e723697",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question:</b> What conclusions do you draw from the pruned model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d199450-d7a8-4494-84b6-b5084c326b9d",
   "metadata": {},
   "source": [
    "## Final model\n",
    "\n",
    "This mathematical formula is equivalent to the NorthPointe model which, based on 137 variables, predicts whether a person will reoffend. \n",
    "\n",
    "<img src=\"pics/compas_diagram.png\" alt=\"COMPAS model as a graph\" class=\"bg-primary\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a1ebd-5289-4fa9-b3a9-057708e0de66",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* Any model built on historical data is going to reproduce structural inequalities.\n",
    "\n",
    "* Even by minimising the variables they are mediated (causal inference term) by \"race\".\n",
    "\n",
    "* Would you buy an intelligent tool that predicts behaviour from 137 variables, what if it has 2 variables?\n",
    "\n",
    "* There are other more important underlying questions about what we mean by justice and the impossibility of implementing it algorithmically if we consider that any AI tool does not incorporate context into its decisions.\n",
    "\n",
    "# References\n",
    "* Angwin, J., & Larson, J. (2016, diciembre 30). Bias in Criminal Risk Scores Is Mathematically Inevitable, Researchers Say. ProPublica. <https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say>\n",
    "* Larson, J., & Angwin, J. (2016, mayo 23). How We Analyzed the COMPAS Recidivism Algorithm. ProPublica. <https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm>\n",
    "* Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell 1, 206â€“215 (2019). <https://doi.org/10.1038/s42256-019-0048-x>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4ca5d-e5d6-442a-a2aa-9726031b4a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b874b-6f95-45a6-95dd-6afe4f29b5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4289e1-a3a4-4061-b745-dbd31759a121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89038cb-f40b-470d-a474-7499e4e330ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
